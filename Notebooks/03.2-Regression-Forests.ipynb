{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was put together by [Jake Vanderplas](http://www.vanderplas.com). Source and license info is on [GitHub](https://github.com/jakevdp/sklearn_tutorial/).</i>. Revised by Lei Qian</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning In-Depth: Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we saw a powerful discriminative classifier, **Support Vector Machines**.\n",
    "Here we'll take a look at motivating another powerful algorithm. This one is a *non-parametric* algorithm called **Random Forests**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Random Forests: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests are an example of an *ensemble learner* built on decision trees.\n",
    "For this reason we'll start by discussing decision trees themselves.\n",
    "\n",
    "Decision trees are extremely intuitive ways to classify or label objects: you simply ask a series of questions designed to zero-in on the classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example_decision_tree():\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    ax = fig.add_axes([0, 0, 0.8, 1], frameon=False, xticks=[], yticks=[])\n",
    "    ax.set_title('Example Decision Tree: Animal Classification', size=24)\n",
    "\n",
    "    def text(ax, x, y, t, size=20, **kwargs):\n",
    "        ax.text(x, y, t,\n",
    "                ha='center', va='center', size=size,\n",
    "                bbox=dict(boxstyle='round', ec='k', fc='w'), **kwargs)\n",
    "\n",
    "    text(ax, 0.5, 0.9, \"How big is\\nthe animal?\", 20)\n",
    "    text(ax, 0.3, 0.6, \"Does the animal\\nhave horns?\", 18)\n",
    "    text(ax, 0.7, 0.6, \"Does the animal\\nhave two legs?\", 18)\n",
    "    text(ax, 0.12, 0.3, \"Are the horns\\nlonger than 10cm?\", 14)\n",
    "    text(ax, 0.38, 0.3, \"Is the animal\\nwearing a collar?\", 14)\n",
    "    text(ax, 0.62, 0.3, \"Does the animal\\nhave wings?\", 14)\n",
    "    text(ax, 0.88, 0.3, \"Does the animal\\nhave a tail?\", 14)\n",
    "\n",
    "    text(ax, 0.4, 0.75, \"> 1m\", 12, alpha=0.4)\n",
    "    text(ax, 0.6, 0.75, \"< 1m\", 12, alpha=0.4)\n",
    "\n",
    "    text(ax, 0.21, 0.45, \"yes\", 12, alpha=0.4)\n",
    "    text(ax, 0.34, 0.45, \"no\", 12, alpha=0.4)\n",
    "\n",
    "    text(ax, 0.66, 0.45, \"yes\", 12, alpha=0.4)\n",
    "    text(ax, 0.79, 0.45, \"no\", 12, alpha=0.4)\n",
    "\n",
    "    ax.plot([0.3, 0.5, 0.7], [0.6, 0.9, 0.6], '-k')\n",
    "    ax.plot([0.12, 0.3, 0.38], [0.3, 0.6, 0.3], '-k')\n",
    "    ax.plot([0.62, 0.7, 0.88], [0.3, 0.6, 0.3], '-k')\n",
    "    ax.plot([0.0, 0.12, 0.20], [0.0, 0.3, 0.0], '--k')\n",
    "    ax.plot([0.28, 0.38, 0.48], [0.0, 0.3, 0.0], '--k')\n",
    "    ax.plot([0.52, 0.62, 0.72], [0.0, 0.3, 0.0], '--k')\n",
    "    ax.plot([0.8, 0.88, 1.0], [0.0, 0.3, 0.0], '--k')\n",
    "    ax.axis([0, 1, 0, 1])\n",
    "plot_example_decision_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary splitting makes this extremely efficient.\n",
    "As always, though, the trick is to *ask the right questions*.\n",
    "This is where the algorithmic process comes in: in training a decision tree classifier, the algorithm looks at the features and decides which questions (or \"splits\") contain the most information.\n",
    "\n",
    "### Creating a Decision Tree\n",
    "\n",
    "Here's an example of a decision tree classifier in scikit-learn. We'll start by defining some two-dimensional labeled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4,\n",
    "                  random_state=0, cluster_std=1.0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some convenience functions in the repository that help "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fig_code1 import visualize_tree, plot_tree_interactive\n",
    "def visualize_tree(estimator, X, y, boundaries=True,\n",
    "                   xlim=None, ylim=None):\n",
    "    estimator.fit(X, y)\n",
    "\n",
    "    if xlim is None:\n",
    "        xlim = (X[:, 0].min() - 0.1, X[:, 0].max() + 0.1)\n",
    "    if ylim is None:\n",
    "        ylim = (X[:, 1].min() - 0.1, X[:, 1].max() + 0.1)\n",
    "\n",
    "    x_min, x_max = xlim\n",
    "    y_min, y_max = ylim\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, alpha=0.2, cmap='rainbow')\n",
    "    plt.clim(y.min(), y.max())\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)        \n",
    "    plt.clim(y.min(), y.max())\n",
    "    \n",
    "    # Plot the decision boundaries\n",
    "    def plot_boundaries(i, xlim, ylim):\n",
    "        if i < 0:\n",
    "            return\n",
    "\n",
    "        tree = estimator.tree_\n",
    "        \n",
    "        if tree.feature[i] == 0:\n",
    "            plt.plot([tree.threshold[i], tree.threshold[i]], ylim, '-k')\n",
    "            plot_boundaries(tree.children_left[i],\n",
    "                            [xlim[0], tree.threshold[i]], ylim)\n",
    "            plot_boundaries(tree.children_right[i],\n",
    "                            [tree.threshold[i], xlim[1]], ylim)\n",
    "        \n",
    "        elif tree.feature[i] == 1:\n",
    "            plt.plot(xlim, [tree.threshold[i], tree.threshold[i]], '-k')\n",
    "            plot_boundaries(tree.children_left[i], xlim,\n",
    "                            [ylim[0], tree.threshold[i]])\n",
    "            plot_boundaries(tree.children_right[i], xlim,\n",
    "                            [tree.threshold[i], ylim[1]])\n",
    "            \n",
    "    if boundaries:\n",
    "        plot_boundaries(0, plt.xlim(), plt.ylim())\n",
    "\n",
    "\n",
    "def plot_tree_interactive(X, y):\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    def interactive_tree(depth=1):\n",
    "        clf = DecisionTreeClassifier(max_depth=depth, random_state=0)\n",
    "        visualize_tree(clf, X, y)\n",
    "\n",
    "    from ipywidgets import interact\n",
    "    return interact(interactive_tree, depth=(1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using IPython's ``interact`` (available in IPython 2.0+, and requires a live kernel) we can view the decision tree splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree_interactive(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that at each increase in depth, every node is split in two **except** those nodes which contain only a single class.\n",
    "The result is a very fast **non-parametric** classification, and can be extremely useful in practice.\n",
    "\n",
    "**Question: Do you see any problems with this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees and over-fitting\n",
    "\n",
    "One issue with decision trees is that it is very easy to create trees which **over-fit** the data. That is, they are flexible enough that they can learn the structure of the noise in the data rather than the signal! For example, take a look at two trees built on two subsets of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "plt.figure()\n",
    "visualize_tree(clf, X[:200], y[:200], boundaries=False)\n",
    "plt.figure()\n",
    "visualize_tree(clf, X[-200:], y[-200:], boundaries=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The details of the classifications are completely different! That is an indication of **over-fitting**: when you predict the value for a new point, the result is more reflective of the noise in the model rather than the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles of Estimators: Random Forests\n",
    "\n",
    "One possible way to address over-fitting is to use an **Ensemble Method**: this is a meta-estimator which essentially averages the results of many individual estimators which over-fit the data. Somewhat surprisingly, the resulting estimates are much more robust and accurate than the individual estimates which make them up!\n",
    "\n",
    "One of the most common ensemble methods is the **Random Forest**, in which the ensemble is made up of many decision trees which are in some way perturbed.\n",
    "\n",
    "There are volumes of theory and precedent about how to randomize these trees, but as an example, let's imagine an ensemble of estimators fit on subsets of the data. We can get an idea of what these might look like as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_randomized_tree(random_state=0):\n",
    "    X, y = make_blobs(n_samples=300, centers=4,\n",
    "                      random_state=0, cluster_std=2.0)\n",
    "    clf = DecisionTreeClassifier(max_depth=15)\n",
    "    \n",
    "    rng = np.random.RandomState(random_state)\n",
    "    i = np.arange(len(y))\n",
    "    rng.shuffle(i)\n",
    "    visualize_tree(clf, X[i[:250]], y[i[:250]], boundaries=False,\n",
    "                   xlim=(X[:, 0].min(), X[:, 0].max()),\n",
    "                   ylim=(X[:, 1].min(), X[:, 1].max()))\n",
    "    \n",
    "from ipywidgets import interact\n",
    "interact(fit_randomized_tree, random_state=(0, 100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the details of the model change as a function of the sample, while the larger characteristics remain the same!\n",
    "The random forest classifier will do something similar to this, but use a combined version of all these trees to arrive at a final answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "visualize_tree(clf, X, y, boundaries=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By averaging over 100 randomly perturbed models, we end up with an overall model which is a much better fit to our data!\n",
    "\n",
    "*(Note: above we randomized the model through sub-sampling... Random Forests use more sophisticated means of randomization, which you can read about in, e.g. the [scikit-learn documentation](http://scikit-learn.org/stable/modules/ensemble.html#forest)*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Example: Moving to Regression\n",
    "\n",
    "Above we were considering random forests within the context of classification.\n",
    "Random forests can also be made to work in the case of regression (that is, continuous rather than categorical variables). The estimator to use for this is ``sklearn.ensemble.RandomForestRegressor``.\n",
    "\n",
    "Let's quickly demonstrate how this can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "x = 10 * np.random.rand(100)\n",
    "\n",
    "def model(x, sigma=0.3):\n",
    "    fast_oscillation = np.sin(5 * x)\n",
    "    slow_oscillation = np.sin(0.5 * x)\n",
    "    noise = sigma * np.random.randn(len(x))\n",
    "\n",
    "    return slow_oscillation + fast_oscillation + noise\n",
    "\n",
    "y = model(x)\n",
    "plt.errorbar(x, y, 0.3, fmt='o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = RandomForestRegressor(100).fit(x[:, None], y).predict(xfit[:, None])\n",
    "ytrue = model(xfit, 0)\n",
    "\n",
    "plt.errorbar(x, y, 0.3, fmt='o')\n",
    "plt.plot(xfit, yfit, '-r');\n",
    "plt.plot(xfit, ytrue, '-k', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the non-parametric random forest model is flexible enough to fit the multi-period data, without us even specifying a multi-period model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Random Forest for Classifying Digits\n",
    "\n",
    "We previously saw the **hand-written digits** data. Let's use that here to test the efficacy of the SVM and Random Forest classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "y = digits.target\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remind us what we're looking at, we'll visualize the first few data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the figure\n",
    "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "# plot the digits: each image is 8x8 pixels\n",
    "for i in range(64):\n",
    "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
    "    \n",
    "    # label the image with the target value\n",
    "    ax.text(0, 7, str(digits.target[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly classify the digits using a decision tree as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)\n",
    "clf = DecisionTreeClassifier(max_depth=11)\n",
    "clf.fit(Xtrain, ytrain)\n",
    "ypred = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the accuracy of this classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(ypred, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for good measure, plot the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(metrics.confusion_matrix(ypred, ytest), cmap='Blues', interpolation='nearest')\n",
    "plt.grid(False)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"predicted label\")\n",
    "plt.ylabel(\"true label\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. Repeat this classification task with ``sklearn.ensemble.RandomForestClassifier``.  How does the ``max_depth``, ``max_features``, and ``n_estimators`` affect the results?\n",
    "2. Try this classification with ``sklearn.svm.SVC``, adjusting ``kernel``, ``C``, and ``gamma``. Which classifier performs optimally?\n",
    "3. Try a few sets of parameters for each model and check the F1 score (``sklearn.metrics.f1_score``) on your results. What's the best F1 score you can reach?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
